#!/bin/bash -l
#
# Script to run shark under a queueing system
#
# ICRAR - International Centre for Radio Astronomy Research
# (c) UWA - The University of Western Australia, 2018
# Copyright by UWA (in the framework of the ICRAR)
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
#

info() {
	echo "shark-run: $1"
}

error() {
	echo "shark-run: ERROR: $1" 1>&2
}

warning() {
	echo "shark-run: WARNING: $1" 1>&2
}

repeat() {
	_all_vals=""
	for x in `eval echo {1..$2}`
	do
		_all_vals="$_all_vals $1"
	done
	echo "${_all_vals## }"
}

# Turns SLURM quantities specifications to lists of individual values
# For example:
#
# "48(x3),24" --> "48 48 48 24"
_slurm_quantity_as_list() {
	_comma_separated_vals="${1//,/ }"
	_all_vals=""
	for val in $_comma_separated_vals
	do
		if [[ "$val" =~ ^([0-9]+)\(x([0-9]+)\) ]]
		then
			_amount=${BASH_REMATCH[1]}
			_rep=${BASH_REMATCH[2]^*}
			_all_vals="$_all_vals `repeat $_amount $_rep`"
		else
			_all_vals="$_all_vals $val"
		fi
	done

	echo "${_all_vals## }"
}

# Returns a list with the number of CPUs that each task will use
#
# CPUs is given explicitly (via -c in shark-submit); otherwise we
# take what we are given on each node
_slurm_cpus_per_task() {
	cpus_per_node=(`_slurm_quantity_as_list $SLURM_JOB_CPUS_PER_NODE`)
	info "CPUs per node: ${cpus_per_node[*]}" >&2
	cpus_per_task=""
	for i in `eval echo {1..${#cpus_per_node[*]}}`
	do
		let "i = i - 1"
		c=${cpus_per_node[i]}
		t=${tasks_per_node[i]}
		div=$(( $c / $t ))
		rem=$(( $c % $t ))

		# Distribute CPUs within a node in round-robin fashion
		if [ $rem = 0 ]
		then
			cpus_per_task="$cpus_per_task `repeat $div $t`"
		else
			cpus_per_task="$cpus_per_task `repeat $div $(($t - $rem))` `repeat $(( $div + 1 )) $rem`"
		fi
	done
	echo "${cpus_per_task}"
}

# Returns a list with the amount of memory that each task will use
#
# Memory is given either per CPU or per node
# In the former case, mem/task = mem/cpu * cpus/task
# In the latter, mem/task = (mem/node) / (task/node)
_slurm_mem_per_task() {
	if [ -z "$SLURM_MEM_PER_CPU" ]
	then
		if [ -z "$SLURM_MEM_PER_NODE" ]
		then
			error "Neither SLURM_MEM_PER_CPU nor SLURM_MEM_PER_NODE could be found, cannot calculate memory per shark instance"
			exit 1
		fi

		mem_per_node=(`_slurm_quantity_as_list $SLURM_MEM_PER_NODE`)
		if [ ${#mem_per_node[*]} = 1 ]
		then
			mem_per_node=(`repeat $mem_per_node $SLURM_JOB_NUM_NODES`)
		fi
		info "Memory per node: ${mem_per_node[*]}" >&2

		for i in `eval echo {1..${#mem_per_node[*]}}`
		do
			let "i = i - 1"
			t=${tasks_per_node[i]}
			m=${mem_per_node[i]}
			mem_per_task="$mem_per_task `repeat $(($m / $t)) $t`"
		done
	else
		if [ $num_particles -eq 0 ]
		then
			mem_per_cpu=(`repeat $SLURM_MEM_PER_CPU ${#shark_subvolumes[*]}`)
		else	
			mem_per_cpu=(`repeat $SLURM_MEM_PER_CPU ${#shark_params[*]}`)
		fi
		for i in `eval echo {1..${#mem_per_cpu[*]}}`
		do
			let "i = i - 1"
			m=${mem_per_cpu[i]}
			c=${cpus_per_task[i]}
			mem_per_task="$mem_per_task $(($m * $c))"
		done
	fi
	echo "${mem_per_task}"
}

print_usage() {
	echo
	echo "$0: Runs shark under a queueing environment"
	echo
	echo "Usage: $0 [-h] [-?] [-v verbosity-level] [-m modules] [-S shark_binary] -V subvolumes"
	echo "       config_file"
	echo
	echo " -h, -?: Show this help"
	echo " -v verbosity-level: shark verbosity"
	echo " -m modules: colon-separted list of modules to load before running shark"
	echo " -S: The shark binary to run. Defaults to standard PATH lookup"
	echo " -V: Space-separated list of subvolumes to process. Can contain ranges like 1-10"
	echo " config_file: The reference configuration file to use for this shark execution"
}
# Check we are under the influence of a queueing system
if [ -z "${SLURM_JOB_ID}" ]
then
	warning ""
	warning "THIS IS NOT MEANT TO HAPPEN"
	warning ""
	warning "You seem to be manually running this script, but this script is designed to be run"
	warning "under a queueing system (PBS/Torque or SLURM) in an HPC facility."
	warning ""
	warning "If you are indeed in an HPC center and trying to run shark, you most probably want"
	warning "to run the shark-submit script (which in turn will invoke us internally)."
	warning ""
	warning "Now, if you are really sure of what you are doing, you have nothing to fear..."
	warning ""
fi

# The submit script should have told us where it lives,
# which is needed later on to deduce where the standard
# python plotting scripts live
if [ -z "$SHARK_HPC_DIR" ]
then
	warning "SHARK_HPC_DIR environment variable not set"
	warning
	warning "This variable should have been set by the shark-submit script automatically"
	warning "to properly support running the python standard plot scripts. If for some reason you"
	warning "are running this script manually you will need to export this variable yourself"
	warning "and point it to the hpc/ directory within the shark git repository."
	warning
	warning "Given that this is not the case, this variable will now be defaulted"
	warning "to this script's parent directory, which might or might not be what you need."

	# See shark-submit fot details on this
	this=$0
	if [ -h $0 ]
	then
		this=$(readlink -f $0)
	fi
	p="`dirname $this`"
	SHARK_HPC_DIR="`cd $p; echo $PWD; cd $OLDPWD`"
fi

# Print queueing information for this job
if [ ! -z "$SLURM_JOB_ID" ]
then
	info ""
	info "Dumping SLURM information for this job submission:"
	info ""
	info "Variable                 Description                           Value"
	info "--------------------------------------------------------------------------------"
	info "SLURM_CLUSTER_NAME       The cluster we are running shark on   $SLURM_CLUSTER_NAME"
	info "SLURM_SUBMIT_DIR         Directory the job was submitted from  $SLURM_SUBMIT_DIR"
	info "SLURM_JOB_PARTITION      The partition we are running shark on $SLURM_JOB_PARTITION"
	info "SLURM_JOB_NAME           Name of the job                       $SLURM_JOB_NAME"
	info "SLURM_JOB_ID             This job's unique identifier job      $SLURM_JOB_ID"
	info "SLURM_JOB_NODELIST       Name of nodes assigned                $SLURM_JOB_NODELIST"
	info "SLURM_JOB_NUM_NODES      Number of nodes allocated             $SLURM_JOB_NUM_NODES"
	info "SLURM_JOB_CPUS_PER_NODE  #CPUs per node available              $SLURM_JOB_CPUS_PER_NODE"
	info "SLURM_NTASKS             #Tasks allocated                      $SLURM_NTASKS"
	info "SLURM_TASKS_PER_NODE     #Tasks to run per node                $SLURM_TASKS_PER_NODE"
	info "SLURM_MEM_PER_CPU        #Allocated memory per CPU             $SLURM_MEM_PER_CPU"
	info "SLURM_MEM_PER_NODE       #Allocated memory per node            $SLURM_MEM_PER_NODE"
	info "SLURM_CPUS_PER_TASK      #CPUs to be used by each task         $SLURM_CPUS_PER_TASK"
	info ""
fi

# Default option values
# These are global variables, so we don't need to pass them around all the time
verbose=
module_names=
shark_verbosity=3
shark_binary=
shark_subvolumes=
shark_params=
num_particles=
shark_options=()
shark_plot=
shark_python_exec=python
# Parse command line options
while getopts "h?m:v:S:E:V:o:pP:" opt
do
	case "$opt" in
		[h?])
			print_usage
			exit 0
			;;
		v)
			shark_verbosity="$OPTARG"
			;;
		m)
			module_names="$OPTARG"
			;;
		S)
			shark_binary="$OPTARG"
			;;
		E)
			num_particles="$OPTARG"
			;;
		V)
			shark_subvolumes="$OPTARG"
			;;
		o)
			shark_options+=("$OPTARG")
			;;
		p)
			shark_plot=yes
			;;
		P)
			shark_python_exec="${OPTARG}"
			;;
		*)
			print_usage 1>&2
			exit 1
			;;
	esac
done


if [ -z "$shark_subvolumes" ]
then
	error "Missing -V option with subvolume list"
	print_usage 1>&2
	exit 1
fi
shark_subvolumes=($shark_subvolumes)

if [ ! $num_particles -eq 0 ]
then
	#reading in the parameter values from the ssv file
	INPUT_FILE="/home/msammons/aux/particlePositions.ssv"
	OIFS=$IFS
	IFS=$(echo -en "\n\b")
	count=0
	while read param1
	do
		shark_params[$count]=$param1
		count=$(($count+1))
	done < $INPUT_FILE
fi
IFS=$OIFS

# Positional argument is the configuration file name
if [ $(($# - $OPTIND)) -lt 0 ]
then
	print_usage 1>&2
	exit 1
fi

config_file=${@:$OPTIND:1}

# Which shark binary should be used
shark_binary=${shark_binary:-shark}

# Make sure the configuration file exists before running
if [ ! -f ${config_file} ]
then
	error "File ${config_file} is not an existing (or accessible) file"
	exit 1
fi

# List and load modules
if [ ! -z "$module_names" ]
then
	module_names="${module_names//:/ }"
	info "Loading modules specified by user: $module_names"
	for m in $module_names
	do
		info "Loading module $m"
		module load "$m" || (error "Failed to load module $m" && exit 1)
	done
fi

# If the user requested to plot things at the end of the execution
# we make sure that our shark instances output the required snapshots;
# otherwise the plots are almost doomed to fail. We do this by calculating the
# required snapshots for a given set of redshifts, and appending those
# snapshots to the output_snapshots parameter of the execution group in the
# configuration file. shark can deal with repeated values for that preference,
# so even if the snapshots were listed already there is no harm done
#
# For the time being we "just know" that the plots will require the snapshots
# corresponding to z=[0, 0.5, 1, 2, 3, 4], so we hardcode that list here. In
# the future we could probably embed that information the the plotting modules
# themselves so it isn't required here
if [ -n "${shark_plot}" ]
then
	output_snapshots=`"${shark_python_exec}" "$SHARK_HPC_DIR/../standard_plots/common.py" snapshots "${config_file}" 0 0.5 1 2 3 4`
	info "Ensuring that shark produces snapshots $output_snapshots"
	sed -i "s/^output_snapshots.*/& $output_snapshots/" "${config_file}"
fi

# Calculate which tasks run where, and how many CPUs and how much memory
# of each them can consume based on the requested resources
#
# Because mem_per_task might depend on cpus_per_task we need to
# get the latter first, then the former.
tasks_per_node=(`_slurm_quantity_as_list $SLURM_TASKS_PER_NODE`)
info "Tasks per node: ${tasks_per_node[*]}"
cpus_per_task=(`_slurm_cpus_per_task`)
info "CPUs per task: ${cpus_per_task[*]}"
mem_per_task=(`_slurm_mem_per_task`)
info "Memory per task: ${mem_per_task[*]}"

# Calculate the command to run
# If possible, we time each execution (but it's not really necessary)
cmd="srun --exclusive -n 1 -N 1"
post_cmd=""
if [ -n "$(command -v time 2> /dev/null)" -a "`type -t time`" = "file" ]
then
	post_cmd="time"
fi
post_cmd="$post_cmd $shark_binary "
for o in ${shark_options[*]}
do
	post_cmd="$post_cmd -o $o"
done
post_cmd="$post_cmd -v $shark_verbosity $config_file"

if [ $num_particles -eq 0 ]
then
	info "Processing ${#shark_subvolumes[*]} tasks: ${shark_subvolumes[*]}"
else
	info "Processing ${#shark_params[*]} tasks: ${shark_params[*]}"
fi
info "================================================================================"
info "Starting shark via srun with command-line:"
info "$cmd -c <threads> --mem-per-cpu <mem> $post_cmd -o <subvol> -t <threads>"
info "================================================================================"
pids=()
if [ $num_particles -eq 0 ]
then
	for i in `eval echo {1..${#shark_subvoumes[*]}}`
	do
		let "i = i - 1"
		s=${shark_subvolumes[i]}
		c=${cpus_per_task[i]}
		m=${mem_per_task[i]}
		_cmd="$cmd -c $c --mem-per-cpu $(($m/$c)) $post_cmd -o \"execution.simulation_batches=$s\" -t $c"
		info "Spawning shark run for subvolume $s with $c threads and $m MB of memory: $_cmd"
		eval $_cmd &> "shark_subvol_${s}.log" &
		pids+=($!)
	done
else 
	for i in `eval echo {1..${#shark_params[*]}}`
	do
		let "i = i - 1"
		s="${shark_subvolumes[*]}"
		c=${cpus_per_task[i]}
		m=${mem_per_task[i]}
		d=$PWD/$i
		_cmd="$cmd -c $c --mem-per-cpu $(($m/$c)) $post_cmd -o \"execution.simulation_batches=$s\" -o \"execution.output_directory=$d\" -t $c"
		p=${shark_params[i]}
		_cmd="$_cmd $p"
		echo $_cmd 
		info "Spawning shark run for subvolume $s and $p with $c threads and $m MB of memory: $_cmd"
		eval $_cmd &> "particle_$i.log" &
		pids+=($!)
	done
fi

info "Waiting for all instances to finish"
all_good=1
for pid in ${pids[*]}
do
	wait $pid
	if [ $? -ne 0 ]
	then
		all_good=0
	fi
done

if [ $all_good = 0 ]
then
	error "Some (or all) of the shark instances exited with an error"
	error "Check the individual output files for details"
	exit 1
fi
info "All shark instances exited cleanly! :)"

# User requested a plot
if [ -n "${shark_plot}" ]
then

	# For the time being we are assuming this script
	# is alongside the rest of the shark sources
	export MPLBACKEND=agg
	"${shark_python_exec}" "$SHARK_HPC_DIR/../standard_plots/all.py" \
	    -c "${config_file}" -v "${shark_subvolumes[*]}" &> shark-plots.log
	if [ $? -ne 0 ]
	then
		warning "An error occurred while producing the plots, please check the shark-plots.log file for details"
	else
		# Find where the plots were produced and copy them
		# to a local plots/ folder for easy access
		info "shark plots successfully produced"
		plots_dir=`"${shark_python_exec}" "$SHARK_HPC_DIR/../standard_plots/common.py" output_dir "${config_file}"`
		mkdir plots
		cp ${plots_dir}/* plots
		info "shark plots successfully copied into plots/"
	fi
fi

info "Job done, bye-bye!"
